{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "970c6259-d875-40f8-ac08-7059d5d24b11",
   "metadata": {},
   "source": [
    "Subword-Based Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e6da4f9-737f-4f3d-9f5a-82746d1183f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in e:\\python\\python312\\lib\\site-packages (3.8.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in e:\\python\\python312\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in e:\\python\\python312\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in e:\\python\\python312\\lib\\site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in e:\\python\\python312\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in e:\\python\\python312\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in e:\\python\\python312\\lib\\site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in e:\\python\\python312\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in e:\\python\\python312\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in e:\\python\\python312\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in e:\\python\\python312\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in e:\\python\\python312\\lib\\site-packages (from spacy) (0.15.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in e:\\python\\python312\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in e:\\python\\python312\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in e:\\python\\python312\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in e:\\python\\python312\\lib\\site-packages (from spacy) (2.10.6)\n",
      "Requirement already satisfied: jinja2 in e:\\python\\python312\\lib\\site-packages (from spacy) (3.1.5)\n",
      "Requirement already satisfied: setuptools in e:\\python\\python312\\lib\\site-packages (from spacy) (75.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\python\\python312\\lib\\site-packages (from spacy) (24.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in e:\\python\\python312\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in e:\\python\\python312\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in e:\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in e:\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in e:\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in e:\\python\\python312\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in e:\\python\\python312\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in e:\\python\\python312\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in e:\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in e:\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in e:\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in e:\\python\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in e:\\python\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\python\\python312\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in e:\\python\\python312\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in e:\\python\\python312\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in e:\\python\\python312\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Requirement already satisfied: wrapt in e:\\python\\python312\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in e:\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0\n",
      "[notice] To update, run: E:\\python\\python312\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/12.8 MB 653.6 kB/s eta 0:00:20\n",
      "     --- ------------------------------------ 1.0/12.8 MB 10.5 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 3.4/12.8 MB 21.4 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 6.5/12.8 MB 31.6 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 8.7/12.8 MB 37.2 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 11.8/12.8 MB 59.5 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.8/12.8 MB 54.4 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 46.7 MB/s eta 0:00:00\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0\n",
      "[notice] To update, run: E:\\python\\python312\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting de-core-news-sm==3.8.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.8.0/de_core_news_sm-3.8.0-py3-none-any.whl (14.6 MB)\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0\n",
      "[notice] To update, run: E:\\python\\python312\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in e:\\python\\python312\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in e:\\python\\python312\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in e:\\python\\python312\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in e:\\python\\python312\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in e:\\python\\python312\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in e:\\python\\python312\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0\n",
      "[notice] To update, run: E:\\python\\python312\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in e:\\python\\python312\\lib\\site-packages (1.26.4)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.1-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "     ---------------------------------------- 0.0/60.8 kB ? eta -:--:--\n",
      "     --------------------------------- ------ 51.2/60.8 kB 2.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 60.8/60.8 kB 1.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: joblib>=1.2.0 in e:\\python\\python312\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.6.1-cp312-cp312-win_amd64.whl (11.1 MB)\n",
      "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.0/11.1 MB 31.7 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 3.2/11.1 MB 41.4 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 5.9/11.1 MB 47.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.1/11.1 MB 52.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.1/11.1 MB 59.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.1/11.1 MB 50.3 MB/s eta 0:00:00\n",
      "Downloading scipy-1.15.1-cp312-cp312-win_amd64.whl (43.6 MB)\n",
      "   ---------------------------------------- 0.0/43.6 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 3.1/43.6 MB 65.6 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 6.0/43.6 MB 54.3 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 8.4/43.6 MB 59.8 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 10.6/43.6 MB 54.4 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 11.8/43.6 MB 50.4 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 12.3/43.6 MB 43.5 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 12.6/43.6 MB 36.4 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 12.9/43.6 MB 31.2 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 13.1/43.6 MB 27.3 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 13.4/43.6 MB 25.2 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 13.8/43.6 MB 21.8 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 14.1/43.6 MB 20.5 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 14.5/43.6 MB 19.3 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 14.9/43.6 MB 17.7 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 15.4/43.6 MB 16.8 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 15.8/43.6 MB 15.2 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 16.3/43.6 MB 14.6 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 16.8/43.6 MB 13.9 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 17.2/43.6 MB 13.4 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 17.8/43.6 MB 12.8 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 18.4/43.6 MB 11.9 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 18.7/43.6 MB 11.5 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 19.3/43.6 MB 11.1 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 20.0/43.6 MB 10.6 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 20.7/43.6 MB 10.2 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 21.4/43.6 MB 10.1 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 22.3/43.6 MB 10.2 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 23.0/43.6 MB 10.6 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 23.7/43.6 MB 11.5 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 24.6/43.6 MB 12.6 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 25.4/43.6 MB 13.4 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 26.4/43.6 MB 14.2 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 27.3/43.6 MB 15.2 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 28.1/43.6 MB 14.9 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 29.0/43.6 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 29.9/43.6 MB 17.2 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 30.9/43.6 MB 18.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 31.8/43.6 MB 18.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 32.7/43.6 MB 18.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 33.6/43.6 MB 19.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 34.2/43.6 MB 18.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 35.1/43.6 MB 18.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 36.0/43.6 MB 18.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 36.6/43.6 MB 17.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 37.9/43.6 MB 19.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 39.0/43.6 MB 19.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 40.2/43.6 MB 20.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 41.4/43.6 MB 20.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  42.6/43.6 MB 20.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  43.6/43.6 MB 21.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  43.6/43.6 MB 21.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 43.6/43.6 MB 18.7 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, scikit-learn\n",
      "Successfully installed scikit-learn-1.6.1 scipy-1.15.1 threadpoolctl-3.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0\n",
      "[notice] To update, run: E:\\python\\python312\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!\"E:\\python\\python312\\python.exe\" -m pip install spacy\n",
    "!\"E:\\python\\python312\\python.exe\" -m spacy download en_core_web_sm\n",
    "!\"E:\\python\\python312\\python.exe\" -m spacy download de_core_news_sm\n",
    "!\"E:\\python\\python312\\python.exe\" -m pip install nltk\n",
    "!\"E:\\python\\python312\\python.exe\" -m pip install numpy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0494a180-21d2-49aa-9b14-6ceed1a7c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from transformers import XLNetTokenizer\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext import vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc119915-6b3b-46bb-be07-3540e865f003",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# WordPiece\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer.tokenize(\"If apples are categorized as fruits, what are dogs classified as?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2477fa16-5cf5-4bed-9adb-bbfe45606d21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Unigram and SentencePiece\n",
    "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "tokenizer.tokenize(\"If apples are categorized as fruits, what are dogs classified as?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871ea480-4793-4cbf-adf1-30462485c11e",
   "metadata": {},
   "source": [
    "<span style=\"color:red;font-size:20pt\">PyTorch</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4cd2e7a-6a09-4d4c-9e59-c94c091687e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-',\n",
       " 'decoder',\n",
       " 'uses',\n",
       " 'this',\n",
       " 'condensed',\n",
       " 'information',\n",
       " 'to',\n",
       " 'recreate',\n",
       " 'original',\n",
       " 'data']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PyTorch\n",
    "dataset = [\n",
    "    (1, \"VAE (Variational AutoEncoder)\"),\n",
    "    (2, \"Operates on Encoder Decoder framework.\"),\n",
    "\t(3, \"- Encoder compresses input data into a simplified abstract space that captures essential characteristics\"),\n",
    "\t(4, \"- Decoder uses this condensed information to recreate original data\"),\n",
    "    (5, \"VAEs focus on learning the underlying patterns within the input data, making it possible to create new data samples that share similar characteristics.\") \n",
    "]\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "tokenizer(dataset[3][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d5b2e46-ea0e-4436-a59f-c3c386dd245e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f20c887f-8f37-4d2f-beb7-156fb513aa1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'new': 30,\n",
       " 'compresses': 19,\n",
       " '.': 3,\n",
       " '<unk>': 0,\n",
       " 'create': 21,\n",
       " 'characteristics': 4,\n",
       " 'data': 1,\n",
       " 'share': 37,\n",
       " '-': 2,\n",
       " 'encoder': 6,\n",
       " 'decoder': 5,\n",
       " 'input': 7,\n",
       " 'it': 27,\n",
       " 'on': 8,\n",
       " 'a': 15,\n",
       " 'that': 9,\n",
       " 'the': 10,\n",
       " ')': 13,\n",
       " 'to': 11,\n",
       " '(': 12,\n",
       " 'framework': 24,\n",
       " ',': 14,\n",
       " 'variational': 46,\n",
       " 'abstract': 16,\n",
       " 'original': 32,\n",
       " 'autoencoder': 17,\n",
       " 'captures': 18,\n",
       " 'underlying': 42,\n",
       " 'condensed': 20,\n",
       " 'essential': 22,\n",
       " 'focus': 23,\n",
       " 'information': 25,\n",
       " 'into': 26,\n",
       " 'simplified': 39,\n",
       " 'learning': 28,\n",
       " 'making': 29,\n",
       " 'operates': 31,\n",
       " 'patterns': 33,\n",
       " 'within': 47,\n",
       " 'possible': 34,\n",
       " 'recreate': 35,\n",
       " 'vaes': 45,\n",
       " 'samples': 36,\n",
       " 'similar': 38,\n",
       " 'uses': 43,\n",
       " 'space': 40,\n",
       " 'this': 41,\n",
       " 'vae': 44}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_ix = vocab.build_vocab_from_iterator(yield_tokens(dataset), specials=[\"<unk>\"])\n",
    "vocab_ix.set_default_index(vocab_ix[\"<unk>\"])\n",
    "vocab_ix.get_stoi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2331a6f-e7e7-496d-8c95-97a75be532a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_sentence_and_indices(itr):\n",
    "    tokenized_sentence = next(itr)\n",
    "    token_indices = [vocab_ix[token] for token in tokenized_sentence]\n",
    "    return tokenized_sentence, token_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "89167a04-d074-4dbb-b5a5-971651d7d869",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_iterator = yield_tokens(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c4901cb4-66cd-486b-8328-dd3463b35de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized sentence: ['-', 'decoder', 'uses', 'this', 'condensed', 'information', 'to', 'recreate', 'original', 'data']\n",
      "token indices: [2, 5, 43, 41, 20, 25, 11, 35, 32, 1]\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentence, token_indices = get_tokenized_sentence_and_indices(my_iterator)\n",
    "# next(my_iterator)\n",
    "print(f\"tokenized sentence: {tokenized_sentence}\")\n",
    "print(f\"token indices: {token_indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2ec8a481-a55b-4cd8-9d22-0aac7d9b2119",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE (Variational AutoEncoder)\n",
      "Operates on Encoder Decoder framework.\n",
      "- Encoder compresses input data into a simplified abstract space that captures essential characteristics\n",
      "- Decoder uses this condensed information to recreate original data\n",
      "VAEs focus on learning the underlying patterns within the input data, making it possible to create new data samples that share similar characteristics.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['<bos>', 'VAE', '(', 'Variational', 'AutoEncoder', ')', '<eos>'],\n",
       " ['<bos>', 'Operates', 'on', 'Encoder', 'Decoder', 'framework', '.', '<eos>'],\n",
       " ['<bos>',\n",
       "  '-',\n",
       "  'Encoder',\n",
       "  'compresses',\n",
       "  'input',\n",
       "  'data',\n",
       "  'into',\n",
       "  'a',\n",
       "  'simplified',\n",
       "  'abstract',\n",
       "  'space',\n",
       "  'that',\n",
       "  'captures',\n",
       "  'essential',\n",
       "  'characteristics',\n",
       "  '<eos>'],\n",
       " ['<bos>',\n",
       "  '-',\n",
       "  'Decoder',\n",
       "  'uses',\n",
       "  'this',\n",
       "  'condensed',\n",
       "  'information',\n",
       "  'to',\n",
       "  'recreate',\n",
       "  'original',\n",
       "  'data',\n",
       "  '<eos>'],\n",
       " ['<bos>',\n",
       "  'VAEs',\n",
       "  'focus',\n",
       "  'on',\n",
       "  'learning',\n",
       "  'the',\n",
       "  'underlying',\n",
       "  'patterns',\n",
       "  'within',\n",
       "  'the',\n",
       "  'input',\n",
       "  'data',\n",
       "  ',',\n",
       "  'making',\n",
       "  'it',\n",
       "  'possible',\n",
       "  'to',\n",
       "  'create',\n",
       "  'new',\n",
       "  'data',\n",
       "  'samples',\n",
       "  'that',\n",
       "  'share',\n",
       "  'similar',\n",
       "  'characteristics',\n",
       "  '.',\n",
       "  '<eos>']]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_en = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "tokens = []\n",
    "max_length = 0\n",
    "\n",
    "for line in dataset:\n",
    "    print(line[1])\n",
    "    tokenized_line = tokenizer_en(line[1])\n",
    "    tokenized_line = ['<bos>'] + tokenized_line + ['<eos>']\n",
    "    tokens.append(tokenized_line)\n",
    "    max_length = max(max_length, len(tokenized_line))\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd1d769-1899-424e-b696-bb86615dfc91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb13a68a-4649-4614-85e8-f9c62b4f0709",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:20pt\">NLTK</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bde5c3d-00ff-42b2-940c-b5b2e9841ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DEC0D\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\DEC0D\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('punkt_tab')\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.util import ngrams\n",
    "from transformers import BertTokenizer\n",
    "from transformers import XLNetTokenizer\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc718278-7cd5-4396-aac8-119dda391889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ocean', 'Temperatures', 'are', 'Rising', 'Much', 'Faster', 'Than', 'Scientists', 'Expected', '.', 'And', 'they', 'are', 'not', 'slowing', 'down', 'anytime', 'soon', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Ocean Temperatures are Rising Much Faster Than Scientists Expected. And they are not slowing down anytime soon.\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4649b99b-6f2a-4957-a7a7-422fd3c5f0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'could', \"n't\", 'help', 'the', 'dog', '.', 'Ca', \"n't\", 'you', 'do', 'it', '?', 'Do', \"n't\", 'be', 'afraid', 'if', 'you', 'are', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"I couldn't help the dog. Can't you do it? Don't be afraid if you are.\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69de5c5e-8082-45a9-a48c-44c68476687a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e186922b-3ce8-4146-a133-eedfa80296ac",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:20pt\">spaCy</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1536020-c550-4f36-90e1-cba85cef8fab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Unicorns', 'are', 'real', '.', 'I', 'saw', 'a', 'unicorn', 'yesterday', '.']\n",
      "Unicorns PROPN nsubj\n",
      "are AUX ROOT\n",
      "real ADJ acomp\n",
      ". PUNCT punct\n",
      "I PRON nsubj\n",
      "saw VERB ROOT\n",
      "a DET det\n",
      "unicorn NOUN dobj\n",
      "yesterday NOUN npadvmod\n",
      ". PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "text = \"I couldn't help the dog. Can't you do it? Don't be afraid if you are.\"\n",
    "text = \"Unicorns are real. I saw a unicorn yesterday.\"\n",
    "# text = \"Ocean Temperatures are Rising Much Faster Than Scientists Expected. And they are not slowing down anytime soon.\"\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "\n",
    "# Making a list of the tokens and printing the list\n",
    "token_list = [token.text for token in doc]\n",
    "print(\"Tokens:\", token_list)\n",
    "\n",
    "# Showing token details\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426e4d12-bb15-4451-9355-ce1e99265999",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3223f4bd-b9f2-4ae6-9f66-9e973ff5fa91",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:20pt\">WordPiece</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52c27ca7-4ef9-48f2-b81f-50045ed9b1aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ibm', 'taught', 'me', 'token', '##ization', '.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer.tokenize(\"IBM taught me tokenization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ae0c12-e96a-417d-bc83-2611cd70b565",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6acca39-2828-44f1-a2e7-1cb662112571",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:20pt\">Unigram</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e5e72f1-98b0-4e73-9072-e589121607ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁IBM', '▁taught', '▁me', '▁token', 'ization', '.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "tokenizer.tokenize(\"IBM taught me tokenization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d726e1-3332-49b5-9883-4d755758419a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2163f12a-0b2a-4a8c-a96b-5254c8272824",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:20pt\">PyTorch</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d449bcf8-7977-4f2e-a1e7-85e5cbb2eb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b77f452b-1b84-43e2-82df-e0b95bb3aa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69a0cfd5-798a-4bdd-a92f-ce5b6cb3f210",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    (1,\"Introduction to NLP\"),\n",
    "    (2,\"Basics of PyTorch\"),\n",
    "    (1,\"NLP Techniques for Text Classification\"),\n",
    "    (3,\"Named Entity Recognition with PyTorch\"),\n",
    "    (3,\"Sentiment Analysis using PyTorch\"),\n",
    "    (3,\"Machine Translation with PyTorch\"),\n",
    "    (1,\" NLP Named Entity,Sentiment Analysis,Machine Translation \"),\n",
    "    (1,\" Machine Translation with NLP \"),\n",
    "    (1,\" Named Entity vs Sentiment Analysis  NLP \")]\n",
    "from torchtext.data.utils import get_tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "62b92561-61af-40d2-bd06-a2db826cac12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data):\n",
    "    for _, text in data:\n",
    "        yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a89b67a-120b-4439-8acc-76cf52c3968b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_iterator = yield_tokens(dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb5a7bae-1fe2-45ce-8271-131526251b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab_from_iterator(yield_tokens(dataset), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d264d766-8a5f-4763-b87b-a2c1378fdd49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentence: ['introduction', 'to', 'nlp']\n",
      "Token Indices: [14, 19, 1]\n"
     ]
    }
   ],
   "source": [
    "def get_tokenized_sentence_and_indices(iterator):\n",
    "    tokenized_sentence = next(iterator)  # Get the next tokenized sentence\n",
    "    token_indices = [vocab[token] for token in tokenized_sentence]  # Get token indices\n",
    "    return tokenized_sentence, token_indices\n",
    "\n",
    "tokenized_sentence, token_indices = get_tokenized_sentence_and_indices(my_iterator)\n",
    "next(my_iterator)\n",
    "\n",
    "print(\"Tokenized Sentence:\", tokenized_sentence)\n",
    "print(\"Token Indices:\", token_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e51eddcb-d8d3-4267-b5f5-884cdef7f59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines after adding special tokens:\n",
      " [['<bos>', 'IBM', 'taught', 'me', 'tokenization', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'], ['<bos>', 'Special', 'tokenizers', 'are', 'ready', 'and', 'they', 'will', 'blow', 'your', 'mind', '<eos>'], ['<bos>', 'just', 'saying', 'hi', '!', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']]\n",
      "Vocabulary: ['<unk>', '<pad>', '<bos>', '<eos>', '!', 'IBM', 'Special', 'and', 'are', 'blow', 'hi', 'just', 'me', 'mind', 'ready', 'saying', 'taught', 'they', 'tokenization', 'tokenizers', 'will', 'your']\n",
      "Token IDs for 'tokenization': {'<bos>': 2, 'blow': 9, '<unk>': 0, 'and': 7, '<eos>': 3, '<pad>': 1, '!': 4, 'will': 20, 'are': 8, 'IBM': 5, 'Special': 6, 'hi': 10, 'just': 11, 'me': 12, 'mind': 13, 'ready': 14, 'saying': 15, 'taught': 16, 'they': 17, 'your': 21, 'tokenization': 18, 'tokenizers': 19}\n"
     ]
    }
   ],
   "source": [
    "lines = [\"IBM taught me tokenization\", \n",
    "         \"Special tokenizers are ready and they will blow your mind\", \n",
    "         \"just saying hi!\"]\n",
    "\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "tokenizer_en = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "tokens = []\n",
    "max_length = 0\n",
    "\n",
    "for line in lines:\n",
    "    tokenized_line = tokenizer_en(line)\n",
    "    tokenized_line = ['<bos>'] + tokenized_line + ['<eos>']\n",
    "    tokens.append(tokenized_line)\n",
    "    max_length = max(max_length, len(tokenized_line))\n",
    "\n",
    "for i in range(len(tokens)):\n",
    "    tokens[i] = tokens[i] + ['<pad>'] * (max_length - len(tokens[i]))\n",
    "\n",
    "print(\"Lines after adding special tokens:\\n\", tokens)\n",
    "\n",
    "# Build vocabulary without unk_init\n",
    "vocab = build_vocab_from_iterator(tokens, specials=['<unk>'])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# Vocabulary and Token Ids\n",
    "print(\"Vocabulary:\", vocab.get_itos())\n",
    "print(\"Token IDs for 'tokenization':\", vocab.get_stoi())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "313edf61-a7cb-4bdc-b78f-ce1440232d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs for new line: [2, 0, 0, 0, 0, 7, 0, 0, 0, 3, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "new_line = \"I learned about embeddings and attention mechanisms.\"\n",
    "\n",
    "# Tokenize the new line\n",
    "tokenized_new_line = tokenizer_en(new_line)\n",
    "tokenized_new_line = ['<bos>'] + tokenized_new_line + ['<eos>']\n",
    "\n",
    "# Pad the new line to match the maximum length of previous lines\n",
    "new_line_padded = tokenized_new_line + ['<pad>'] * (max_length - len(tokenized_new_line))\n",
    "\n",
    "# Convert tokens to IDs and handle unknown words\n",
    "new_line_ids = [vocab[token] if token in vocab else vocab['<unk>'] for token in new_line_padded]\n",
    "\n",
    "# Example usage\n",
    "print(\"Token IDs for new line:\", new_line_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1308e723-e0d5-497e-b9f6-cbc688ca289a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1b120d-aec2-419f-979a-336389f5ee39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c729d4f-4530-44c2-9469-0306d9da5f42",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:20pt\">Comparative text tokenization and performance analysis</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2f6de662-e396-4117-81a3-89beae8a4a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "28777dcf-07cc-4825-b9c1-61310383ea43",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Going through the world of tokenization has been like walking through a huge maze made of words, symbols, and meanings. Each turn shows a bit more about the cool ways computers learn to understand our language. And while I'm still finding my way through it, the journey’s been enlightening and, honestly, a bunch of fun.\n",
    "Eager to see where this learning path takes me next!\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2cb212d7-4bc4-46c7-8595-1540cba85574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_frequencies(tokens, method_name):\n",
    "    print(f\"{method_name} Token Frequencies: {dict(Counter(tokens))}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "20b1dc9e-5483-4115-a63a-e3da9f8a68b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.timedelta(0)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "st_time = datetime.now()\n",
    "nltk_tokens = word_tokenize(text)\n",
    "nltk_time = datetime.now() - st_time\n",
    "nltk_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cd652d3c-e0f1-48d5-ad3e-74b1127d24af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.timedelta(microseconds=30000)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "st_time = datetime.now()\n",
    "spacy_tokens = [token.text for token in nlp(text)]\n",
    "spacy_time = datetime.now() - st_time\n",
    "spacy_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "63a43d77-0bbd-4439-8ec2-f33c60eea20c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.timedelta(microseconds=1998)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BERT\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "st_time = datetime.now()\n",
    "bert_tokens = bert_tokenizer.tokenize(text)\n",
    "bert_time = datetime.now() - st_time\n",
    "bert_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "359054ba-d40a-42ee-933f-0aec7c9e4ba3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.timedelta(0)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XLNet\n",
    "xlnet_tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "st_time = datetime.now()\n",
    "xlnet_tokens = xlnet_tokenizer.tokenize(text)\n",
    "xlnet_time = datetime.now() - st_time\n",
    "xlnet_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1194828f-2db2-470e-91c4-ebcf1ca0251d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Tokens: ['Going', 'through', 'the', 'world', 'of', 'tokenization', 'has', 'been', 'like', 'walking', 'through', 'a', 'huge', 'maze', 'made', 'of', 'words', ',', 'symbols', ',', 'and', 'meanings', '.', 'Each', 'turn', 'shows', 'a', 'bit', 'more', 'about', 'the', 'cool', 'ways', 'computers', 'learn', 'to', 'understand', 'our', 'language', '.', 'And', 'while', 'I', \"'m\", 'still', 'finding', 'my', 'way', 'through', 'it', ',', 'the', 'journey', '’', 's', 'been', 'enlightening', 'and', ',', 'honestly', ',', 'a', 'bunch', 'of', 'fun', '.', 'Eager', 'to', 'see', 'where', 'this', 'learning', 'path', 'takes', 'me', 'next', '!', \"''\"]\n",
      "Time taken: 0:00:00\n",
      "NLTK Token Frequencies: {'Going': 1, 'through': 3, 'the': 3, 'world': 1, 'of': 3, 'tokenization': 1, 'has': 1, 'been': 2, 'like': 1, 'walking': 1, 'a': 3, 'huge': 1, 'maze': 1, 'made': 1, 'words': 1, ',': 5, 'symbols': 1, 'and': 2, 'meanings': 1, '.': 3, 'Each': 1, 'turn': 1, 'shows': 1, 'bit': 1, 'more': 1, 'about': 1, 'cool': 1, 'ways': 1, 'computers': 1, 'learn': 1, 'to': 2, 'understand': 1, 'our': 1, 'language': 1, 'And': 1, 'while': 1, 'I': 1, \"'m\": 1, 'still': 1, 'finding': 1, 'my': 1, 'way': 1, 'it': 1, 'journey': 1, '’': 1, 's': 1, 'enlightening': 1, 'honestly': 1, 'bunch': 1, 'fun': 1, 'Eager': 1, 'see': 1, 'where': 1, 'this': 1, 'learning': 1, 'path': 1, 'takes': 1, 'me': 1, 'next': 1, '!': 1, \"''\": 1}\n",
      "\n",
      "spacy Tokens: ['\\n', 'Going', 'through', 'the', 'world', 'of', 'tokenization', 'has', 'been', 'like', 'walking', 'through', 'a', 'huge', 'maze', 'made', 'of', 'words', ',', 'symbols', ',', 'and', 'meanings', '.', 'Each', 'turn', 'shows', 'a', 'bit', 'more', 'about', 'the', 'cool', 'ways', 'computers', 'learn', 'to', 'understand', 'our', 'language', '.', 'And', 'while', 'I', \"'m\", 'still', 'finding', 'my', 'way', 'through', 'it', ',', 'the', 'journey', '’s', 'been', 'enlightening', 'and', ',', 'honestly', ',', 'a', 'bunch', 'of', 'fun', '.', '\\n', 'Eager', 'to', 'see', 'where', 'this', 'learning', 'path', 'takes', 'me', 'next', '!', '\"', '\\n']\n",
      "Time taken: 0:00:00.030000\n",
      "NLTK Token Frequencies: {'\\n': 3, 'Going': 1, 'through': 3, 'the': 3, 'world': 1, 'of': 3, 'tokenization': 1, 'has': 1, 'been': 2, 'like': 1, 'walking': 1, 'a': 3, 'huge': 1, 'maze': 1, 'made': 1, 'words': 1, ',': 5, 'symbols': 1, 'and': 2, 'meanings': 1, '.': 3, 'Each': 1, 'turn': 1, 'shows': 1, 'bit': 1, 'more': 1, 'about': 1, 'cool': 1, 'ways': 1, 'computers': 1, 'learn': 1, 'to': 2, 'understand': 1, 'our': 1, 'language': 1, 'And': 1, 'while': 1, 'I': 1, \"'m\": 1, 'still': 1, 'finding': 1, 'my': 1, 'way': 1, 'it': 1, 'journey': 1, '’s': 1, 'enlightening': 1, 'honestly': 1, 'bunch': 1, 'fun': 1, 'Eager': 1, 'see': 1, 'where': 1, 'this': 1, 'learning': 1, 'path': 1, 'takes': 1, 'me': 1, 'next': 1, '!': 1, '\"': 1}\n",
      "\n",
      "BERT Tokens: ['going', 'through', 'the', 'world', 'of', 'token', '##ization', 'has', 'been', 'like', 'walking', 'through', 'a', 'huge', 'maze', 'made', 'of', 'words', ',', 'symbols', ',', 'and', 'meanings', '.', 'each', 'turn', 'shows', 'a', 'bit', 'more', 'about', 'the', 'cool', 'ways', 'computers', 'learn', 'to', 'understand', 'our', 'language', '.', 'and', 'while', 'i', \"'\", 'm', 'still', 'finding', 'my', 'way', 'through', 'it', ',', 'the', 'journey', '’', 's', 'been', 'en', '##light', '##ening', 'and', ',', 'honestly', ',', 'a', 'bunch', 'of', 'fun', '.', 'eager', 'to', 'see', 'where', 'this', 'learning', 'path', 'takes', 'me', 'next', '!', '\"']\n",
      "Time taken: 0:00:00.001998\n",
      "NLTK Token Frequencies: {'going': 1, 'through': 3, 'the': 3, 'world': 1, 'of': 3, 'token': 1, '##ization': 1, 'has': 1, 'been': 2, 'like': 1, 'walking': 1, 'a': 3, 'huge': 1, 'maze': 1, 'made': 1, 'words': 1, ',': 5, 'symbols': 1, 'and': 3, 'meanings': 1, '.': 3, 'each': 1, 'turn': 1, 'shows': 1, 'bit': 1, 'more': 1, 'about': 1, 'cool': 1, 'ways': 1, 'computers': 1, 'learn': 1, 'to': 2, 'understand': 1, 'our': 1, 'language': 1, 'while': 1, 'i': 1, \"'\": 1, 'm': 1, 'still': 1, 'finding': 1, 'my': 1, 'way': 1, 'it': 1, 'journey': 1, '’': 1, 's': 1, 'en': 1, '##light': 1, '##ening': 1, 'honestly': 1, 'bunch': 1, 'fun': 1, 'eager': 1, 'see': 1, 'where': 1, 'this': 1, 'learning': 1, 'path': 1, 'takes': 1, 'me': 1, 'next': 1, '!': 1, '\"': 1}\n",
      "\n",
      "XLNetK Tokens: ['▁Going', '▁through', '▁the', '▁world', '▁of', '▁token', 'ization', '▁has', '▁been', '▁like', '▁walking', '▁through', '▁a', '▁huge', '▁maze', '▁made', '▁of', '▁words', ',', '▁symbols', ',', '▁and', '▁meaning', 's', '.', '▁Each', '▁turn', '▁shows', '▁a', '▁bit', '▁more', '▁about', '▁the', '▁cool', '▁ways', '▁computers', '▁learn', '▁to', '▁understand', '▁our', '▁language', '.', '▁And', '▁while', '▁I', \"'\", 'm', '▁still', '▁finding', '▁my', '▁way', '▁through', '▁it', ',', '▁the', '▁journey', '’', 's', '▁been', '▁enlighten', 'ing', '▁and', ',', '▁honestly', ',', '▁a', '▁bunch', '▁of', '▁fun', '.', '▁E', 'ager', '▁to', '▁see', '▁where', '▁this', '▁learning', '▁path', '▁takes', '▁me', '▁next', '!', '\"']\n",
      "Time taken: 0:00:00\n",
      "NLTK Token Frequencies: {'▁Going': 1, '▁through': 3, '▁the': 3, '▁world': 1, '▁of': 3, '▁token': 1, 'ization': 1, '▁has': 1, '▁been': 2, '▁like': 1, '▁walking': 1, '▁a': 3, '▁huge': 1, '▁maze': 1, '▁made': 1, '▁words': 1, ',': 5, '▁symbols': 1, '▁and': 2, '▁meaning': 1, 's': 2, '.': 3, '▁Each': 1, '▁turn': 1, '▁shows': 1, '▁bit': 1, '▁more': 1, '▁about': 1, '▁cool': 1, '▁ways': 1, '▁computers': 1, '▁learn': 1, '▁to': 2, '▁understand': 1, '▁our': 1, '▁language': 1, '▁And': 1, '▁while': 1, '▁I': 1, \"'\": 1, 'm': 1, '▁still': 1, '▁finding': 1, '▁my': 1, '▁way': 1, '▁it': 1, '▁journey': 1, '’': 1, '▁enlighten': 1, 'ing': 1, '▁honestly': 1, '▁bunch': 1, '▁fun': 1, '▁E': 1, 'ager': 1, '▁see': 1, '▁where': 1, '▁this': 1, '▁learning': 1, '▁path': 1, '▁takes': 1, '▁me': 1, '▁next': 1, '!': 1, '\"': 1}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"NLTK Tokens: {nltk_tokens}\\nTime taken: {nltk_time}\")\n",
    "show_frequencies(nltk_tokens, \"NLTK\")\n",
    "print(f\"spacy Tokens: {spacy_tokens}\\nTime taken: {spacy_time}\")\n",
    "show_frequencies(spacy_tokens, \"NLTK\")\n",
    "print(f\"BERT Tokens: {bert_tokens}\\nTime taken: {bert_time}\")\n",
    "show_frequencies(bert_tokens, \"NLTK\")\n",
    "print(f\"XLNetK Tokens: {xlnet_tokens}\\nTime taken: {xlnet_time}\")\n",
    "show_frequencies(xlnet_tokens, \"NLTK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07613e8f-e415-4227-a0eb-1bb0eb990f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
