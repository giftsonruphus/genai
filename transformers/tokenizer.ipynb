{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "970c6259-d875-40f8-ac08-7059d5d24b11",
   "metadata": {},
   "source": [
    "Subword-Based Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8e6da4f9-737f-4f3d-9f5a-82746d1183f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in e:\\python\\python312\\lib\\site-packages (3.8.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in e:\\python\\python312\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in e:\\python\\python312\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in e:\\python\\python312\\lib\\site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in e:\\python\\python312\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in e:\\python\\python312\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in e:\\python\\python312\\lib\\site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in e:\\python\\python312\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in e:\\python\\python312\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in e:\\python\\python312\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in e:\\python\\python312\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in e:\\python\\python312\\lib\\site-packages (from spacy) (0.15.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in e:\\python\\python312\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in e:\\python\\python312\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in e:\\python\\python312\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in e:\\python\\python312\\lib\\site-packages (from spacy) (2.10.6)\n",
      "Requirement already satisfied: jinja2 in e:\\python\\python312\\lib\\site-packages (from spacy) (3.1.5)\n",
      "Requirement already satisfied: setuptools in e:\\python\\python312\\lib\\site-packages (from spacy) (75.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\python\\python312\\lib\\site-packages (from spacy) (24.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in e:\\python\\python312\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in e:\\python\\python312\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in e:\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in e:\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in e:\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in e:\\python\\python312\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in e:\\python\\python312\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in e:\\python\\python312\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in e:\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in e:\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in e:\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in e:\\python\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in e:\\python\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\python\\python312\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in e:\\python\\python312\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in e:\\python\\python312\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in e:\\python\\python312\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Requirement already satisfied: wrapt in e:\\python\\python312\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in e:\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0\n",
      "[notice] To update, run: E:\\python\\python312\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/12.8 MB 991.0 kB/s eta 0:00:13\n",
      "     - -------------------------------------- 0.6/12.8 MB 9.4 MB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 16.3 MB/s eta 0:00:01\n",
      "     --------- ------------------------------ 2.9/12.8 MB 20.8 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 4.4/12.8 MB 23.7 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 5.7/12.8 MB 24.4 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 6.6/12.8 MB 25.0 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 7.9/12.8 MB 25.1 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 8.7/12.8 MB 25.3 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 9.9/12.8 MB 25.3 MB/s eta 0:00:01\n",
      "     ---------------------------------- ---- 11.3/12.8 MB 28.5 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.5/12.8 MB 28.5 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 27.3 MB/s eta 0:00:00\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0\n",
      "[notice] To update, run: E:\\python\\python312\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in e:\\python\\python312\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in e:\\python\\python312\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in e:\\python\\python312\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in e:\\python\\python312\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/1.5 MB 660.6 kB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.1/1.5 MB 1.8 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 0.4/1.5 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.1/1.5 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 8.0 MB/s eta 0:00:00\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "   ---------------------------------------- 0.0/301.8 kB ? eta -:--:--\n",
      "   ---------- ----------------------------- 81.9/301.8 kB ? eta -:--:--\n",
      "   ---------- ----------------------------- 81.9/301.8 kB ? eta -:--:--\n",
      "   ---------- ----------------------------- 81.9/301.8 kB ? eta -:--:--\n",
      "   ---------- ----------------------------- 81.9/301.8 kB ? eta -:--:--\n",
      "   ---------- ----------------------------- 81.9/301.8 kB ? eta -:--:--\n",
      "   ----------- --------------------------- 92.2/301.8 kB 327.7 kB/s eta 0:00:01\n",
      "   -------------- ----------------------- 112.6/301.8 kB 364.4 kB/s eta 0:00:01\n",
      "   --------------- ---------------------- 122.9/301.8 kB 327.4 kB/s eta 0:00:01\n",
      "   -------------------- ----------------- 163.8/301.8 kB 392.8 kB/s eta 0:00:01\n",
      "   --------------------- ---------------- 174.1/301.8 kB 388.2 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 245.8/301.8 kB 485.6 kB/s eta 0:00:01\n",
      "   ------------------------------------ - 286.7/301.8 kB 535.8 kB/s eta 0:00:01\n",
      "   -------------------------------------- 301.8/301.8 kB 533.0 kB/s eta 0:00:00\n",
      "Installing collected packages: joblib, nltk\n",
      "Successfully installed joblib-1.4.2 nltk-3.9.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script nltk.exe is installed in 'E:\\python\\python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0\n",
      "[notice] To update, run: E:\\python\\python312\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!\"E:\\python\\python312\\python.exe\" -m pip install spacy\n",
    "!\"E:\\python\\python312\\python.exe\" -m spacy download en_core_web_sm\n",
    "!\"E:\\python\\python312\\python.exe\" -m pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0494a180-21d2-49aa-9b14-6ceed1a7c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from transformers import XLNetTokenizer\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext import vocab\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc119915-6b3b-46bb-be07-3540e865f003",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# WordPiece\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer.tokenize(\"If apples are categorized as fruits, what are dogs classified as?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2477fa16-5cf5-4bed-9adb-bbfe45606d21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Unigram and SentencePiece\n",
    "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "tokenizer.tokenize(\"If apples are categorized as fruits, what are dogs classified as?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4cd2e7a-6a09-4d4c-9e59-c94c091687e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-',\n",
       " 'decoder',\n",
       " 'uses',\n",
       " 'this',\n",
       " 'condensed',\n",
       " 'information',\n",
       " 'to',\n",
       " 'recreate',\n",
       " 'original',\n",
       " 'data']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PyTorch\n",
    "dataset = [\n",
    "    (1, \"VAE (Variational AutoEncoder)\"),\n",
    "    (2, \"Operates on Encoder Decoder framework.\"),\n",
    "\t(3, \"- Encoder compresses input data into a simplified abstract space that captures essential characteristics\"),\n",
    "\t(4, \"- Decoder uses this condensed information to recreate original data\"),\n",
    "    (5, \"VAEs focus on learning the underlying patterns within the input data, making it possible to create new data samples that share similar characteristics.\") \n",
    "]\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "tokenizer(dataset[3][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d5b2e46-ea0e-4436-a59f-c3c386dd245e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f20c887f-8f37-4d2f-beb7-156fb513aa1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'new': 30,\n",
       " 'compresses': 19,\n",
       " '.': 3,\n",
       " '<unk>': 0,\n",
       " 'create': 21,\n",
       " 'characteristics': 4,\n",
       " 'data': 1,\n",
       " 'share': 37,\n",
       " '-': 2,\n",
       " 'encoder': 6,\n",
       " 'decoder': 5,\n",
       " 'input': 7,\n",
       " 'it': 27,\n",
       " 'on': 8,\n",
       " 'a': 15,\n",
       " 'that': 9,\n",
       " 'the': 10,\n",
       " ')': 13,\n",
       " 'to': 11,\n",
       " '(': 12,\n",
       " 'framework': 24,\n",
       " ',': 14,\n",
       " 'variational': 46,\n",
       " 'abstract': 16,\n",
       " 'original': 32,\n",
       " 'autoencoder': 17,\n",
       " 'captures': 18,\n",
       " 'underlying': 42,\n",
       " 'condensed': 20,\n",
       " 'essential': 22,\n",
       " 'focus': 23,\n",
       " 'information': 25,\n",
       " 'into': 26,\n",
       " 'simplified': 39,\n",
       " 'learning': 28,\n",
       " 'making': 29,\n",
       " 'operates': 31,\n",
       " 'patterns': 33,\n",
       " 'within': 47,\n",
       " 'possible': 34,\n",
       " 'recreate': 35,\n",
       " 'vaes': 45,\n",
       " 'samples': 36,\n",
       " 'similar': 38,\n",
       " 'uses': 43,\n",
       " 'space': 40,\n",
       " 'this': 41,\n",
       " 'vae': 44}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_ix = vocab.build_vocab_from_iterator(yield_tokens(dataset), specials=[\"<unk>\"])\n",
    "vocab_ix.set_default_index(vocab_ix[\"<unk>\"])\n",
    "vocab_ix.get_stoi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2331a6f-e7e7-496d-8c95-97a75be532a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_sentence_and_indices(itr):\n",
    "    tokenized_sentence = next(itr)\n",
    "    token_indices = [vocab_ix[token] for token in tokenized_sentence]\n",
    "    return tokenized_sentence, token_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "89167a04-d074-4dbb-b5a5-971651d7d869",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_iterator = yield_tokens(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c4901cb4-66cd-486b-8328-dd3463b35de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized sentence: ['-', 'decoder', 'uses', 'this', 'condensed', 'information', 'to', 'recreate', 'original', 'data']\n",
      "token indices: [2, 5, 43, 41, 20, 25, 11, 35, 32, 1]\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentence, token_indices = get_tokenized_sentence_and_indices(my_iterator)\n",
    "# next(my_iterator)\n",
    "print(f\"tokenized sentence: {tokenized_sentence}\")\n",
    "print(f\"token indices: {token_indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2ec8a481-a55b-4cd8-9d22-0aac7d9b2119",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE (Variational AutoEncoder)\n",
      "Operates on Encoder Decoder framework.\n",
      "- Encoder compresses input data into a simplified abstract space that captures essential characteristics\n",
      "- Decoder uses this condensed information to recreate original data\n",
      "VAEs focus on learning the underlying patterns within the input data, making it possible to create new data samples that share similar characteristics.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['<bos>', 'VAE', '(', 'Variational', 'AutoEncoder', ')', '<eos>'],\n",
       " ['<bos>', 'Operates', 'on', 'Encoder', 'Decoder', 'framework', '.', '<eos>'],\n",
       " ['<bos>',\n",
       "  '-',\n",
       "  'Encoder',\n",
       "  'compresses',\n",
       "  'input',\n",
       "  'data',\n",
       "  'into',\n",
       "  'a',\n",
       "  'simplified',\n",
       "  'abstract',\n",
       "  'space',\n",
       "  'that',\n",
       "  'captures',\n",
       "  'essential',\n",
       "  'characteristics',\n",
       "  '<eos>'],\n",
       " ['<bos>',\n",
       "  '-',\n",
       "  'Decoder',\n",
       "  'uses',\n",
       "  'this',\n",
       "  'condensed',\n",
       "  'information',\n",
       "  'to',\n",
       "  'recreate',\n",
       "  'original',\n",
       "  'data',\n",
       "  '<eos>'],\n",
       " ['<bos>',\n",
       "  'VAEs',\n",
       "  'focus',\n",
       "  'on',\n",
       "  'learning',\n",
       "  'the',\n",
       "  'underlying',\n",
       "  'patterns',\n",
       "  'within',\n",
       "  'the',\n",
       "  'input',\n",
       "  'data',\n",
       "  ',',\n",
       "  'making',\n",
       "  'it',\n",
       "  'possible',\n",
       "  'to',\n",
       "  'create',\n",
       "  'new',\n",
       "  'data',\n",
       "  'samples',\n",
       "  'that',\n",
       "  'share',\n",
       "  'similar',\n",
       "  'characteristics',\n",
       "  '.',\n",
       "  '<eos>']]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_en = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "tokens = []\n",
    "max_length = 0\n",
    "\n",
    "for line in dataset:\n",
    "    print(line[1])\n",
    "    tokenized_line = tokenizer_en(line[1])\n",
    "    tokenized_line = ['<bos>'] + tokenized_line + ['<eos>']\n",
    "    tokens.append(tokenized_line)\n",
    "    max_length = max(max_length, len(tokenized_line))\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd1d769-1899-424e-b696-bb86615dfc91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
